import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import pandas as pd
import os
from PIL import Image
import numpy as np
from sklearn.metrics import classification_report
from sklearn.model_selection import KFold  # æ–°å¢žï¼šKæŠ˜åˆ†å‰²
import warnings
warnings.filterwarnings('ignore')
import logging
from datetime import datetime
import csv

# ======================== 1. æ ¸å¿ƒé…ç½®ï¼ˆæ–°å¢žKæŠ˜å‚æ•°ï¼‰ ========================
CONFIG = {
    # è·¯å¾„é…ç½®ï¼ˆä¿æŒä½ çš„åŽŸå§‹è·¯å¾„ï¼‰
    'train_csv': '/root/autodl-tmp/dataset/Training_Set/RFMiD_Training_Labels.csv',
    'train_img': '/root/autodl-tmp/dataset/Training_Set/Training',
    'val_csv': '/root/autodl-tmp/dataset/Evaluation_Set/RFMiD_Validation_Labels.csv',
    'val_img': '/root/autodl-tmp/dataset/Evaluation_Set/Validation',
    'test_csv': '/root/autodl-tmp/dataset/Test_Set/RFMiD_Testing_Labels.csv',
    'test_img': '/root/autodl-tmp/dataset/Test_Set/Test',
    
    # å¤šåˆ†ç±»ç›¸å…³é…ç½®
    'all_class_names': [],
    'actual_class_names': [],
    'img_ext': '.png',          
    'img_size': (224, 224),
    'batch_size': 32,
    'num_workers': 16,          
    'epochs': 50,
    'lr': 1e-3,
    'weight_decay': 1e-5,       
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'output_dir': 'multiclass_kfold_results',  # ä¿®æ”¹ç›®å½•å
    'models_to_train': ['resnet50'],  # å¯æ·»åŠ æ›´å¤šæ¨¡åž‹
    'n_splits': 5,  # æ–°å¢žï¼š5æŠ˜äº¤å‰éªŒè¯
}


# ======================== 2. æ—¥å¿—é…ç½®ï¼ˆæ”¯æŒæŠ˜æ•°æ ‡è¯†ï¼‰ ========================
def setup_logger(model_name, fold_idx=None):
    """
    åˆ›å»ºæ¨¡åž‹ä¸“å±žæ—¥å¿—å™¨
    fold_idx: æŠ˜æ•°ç´¢å¼•ï¼ˆNoneè¡¨ç¤ºæ±‡æ€»æ—¥å¿—ï¼‰
    """
    os.makedirs(CONFIG['output_dir'], exist_ok=True)
    log_dir = os.path.join(CONFIG['output_dir'], 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    if fold_idx is not None:
        log_file = os.path.join(log_dir, f'{model_name}_fold{fold_idx+1}_train_{timestamp}.log')
        logger_name = f'{model_name}_fold{fold_idx+1}'
    else:
        log_file = os.path.join(log_dir, f'{model_name}_summary_{timestamp}.log')
        logger_name = f'{model_name}_summary'
    
    logger = logging.getLogger(logger_name)
    logger.setLevel(logging.INFO)
    logger.propagate = False
    
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    return logger, log_file

# ======================== 3. æå–æ‰€æœ‰ç±»åˆ« ========================
def extract_all_classes_from_col3(csv_path, logger):
    df = pd.read_csv(csv_path)
    disease_cols = df.columns[2:].tolist()
    CONFIG['all_class_names'] = ['Normal'] + disease_cols
    logger.info(f"âœ… æå–æ‰€æœ‰ç±»åˆ«ï¼ˆå«æ— æ ·æœ¬ï¼‰ï¼šå…±{len(CONFIG['all_class_names'])}ç±»")
    return df

# ======================== 4. è‡ªå®šä¹‰å¤šåˆ†ç±»æ•°æ®é›†ï¼ˆå¢žå¼ºç‰ˆï¼‰ ========================
class RFMiDMulticlassDataset(Dataset):
    def __init__(self, csv_path, image_dir=None, transform=None, subset_idx=None):
        ### ä¿®å¤ï¼šæ”¯æŒåˆå¹¶CSVè·¯å¾„å’Œå­é›†ç´¢å¼•
        self.csv_df = pd.read_csv(csv_path)
        self.transform = transform
        self.all_class_names = CONFIG['all_class_names']
        self.disease_cols = self.all_class_names[1:]
        
        ### ä¿®å¤ï¼šæ”¯æŒKæŠ˜å­é›†
        if subset_idx is not None:
            self.csv_df = self.csv_df.iloc[subset_idx].reset_index(drop=True)
        
        # æ ‡ç­¾ç”Ÿæˆé€»è¾‘ï¼ˆä¿æŒåŽŸé€»è¾‘ï¼‰
        self.img_ids = self.csv_df['ID'].values
        self.labels = []
        for _, row in self.csv_df.iterrows():
            if row['Disease_Risk'] == 0:
                self.labels.append(0)
            else:
                disease_label = -1
                for cls_idx, cls_name in enumerate(self.disease_cols, 1):
                    if row[cls_name] == 1:
                        disease_label = cls_idx
                        break
                self.labels.append(disease_label if disease_label != -1 else len(self.all_class_names)-1)
        
        # å›¾ç‰‡è·¯å¾„æ˜ å°„ï¼ˆä¼˜åŒ–ï¼šæ”¯æŒå¤šç›®å½•æ‰«æï¼‰
        self.img_path_dict = {}
        img_dirs_to_scan = []
        if image_dir is not None:
            if isinstance(image_dir, (list, tuple)):
                img_dirs_to_scan = list(image_dir)
            else:
                img_dirs_to_scan = [image_dir]
        else:
            img_dirs_to_scan = [CONFIG['train_img'], CONFIG['val_img']]
        
        for img_dir in img_dirs_to_scan:
            if os.path.exists(img_dir):
                self.img_path_dict.update({
                    int(os.path.splitext(f)[0]): os.path.join(img_dir, f)
                    for f in os.listdir(img_dir) if f.endswith(CONFIG['img_ext'])
                })

    def __len__(self):
        return len(self.csv_df)

    def __getitem__(self, idx):
        img_id = self.img_ids[idx]
        img_path = self.img_path_dict[img_id]
        
        try:
            image = Image.open(img_path).convert('RGB')
        except:
            image = Image.fromarray(np.random.randint(0, 255, size=CONFIG['img_size'] + (3,), dtype=np.uint8))
        
        if self.transform:
            image = self.transform(image)
        
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        return image, label

# ======================== 5. æ•°æ®é¢„å¤„ç†/å¢žå¼º ========================
def get_multiclass_transforms(train=True):
    transform_list = [
        transforms.Resize(CONFIG['img_size']),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ]
    
    if train:
        augmentations = [
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=20),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
        ]
        np.random.shuffle(augmentations)
        selected_augs = augmentations[:np.random.randint(2, 4)]
        transform_list = selected_augs + transform_list
    
    return transforms.Compose(transform_list)

# ======================== 6. åˆ›å»ºKæŠ˜DataLoaderï¼ˆæ ¸å¿ƒæ–°å¢žï¼‰ ========================
def create_kfold_dataloaders(logger, n_splits=5):
    """
    åˆ›å»ºKæŠ˜äº¤å‰éªŒè¯çš„æ•°æ®åŠ è½½å™¨
    è¿”å›žï¼škfold_splitter, combined_csv_path, test_loader
    """
    # åˆå¹¶è®­ç»ƒé›†å’ŒéªŒè¯é›†CSV
    train_df = pd.read_csv(CONFIG['train_csv'])
    val_df = pd.read_csv(CONFIG['val_csv'])
    combined_df = pd.concat([train_df, val_df], ignore_index=True)
    
    # ä¿å­˜åˆå¹¶åŽçš„æ•°æ®å¹¶è¿”å›žè·¯å¾„
    combined_csv_path = os.path.join(CONFIG['output_dir'], 'combined_train_val_multiclass.csv')
    combined_df.to_csv(combined_csv_path, index=False)
    logger.info(f"åˆå¹¶è®­ç»ƒ+éªŒè¯é›†ï¼š{len(train_df)} + {len(val_df)} = {len(combined_df)} æ ·æœ¬")
    logger.info(f"åˆå¹¶CSVå·²ä¿å­˜è‡³ï¼š{combined_csv_path}")
    
    # åˆ›å»ºKæŠ˜åˆ†å‰²å™¨
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    split_indices = list(kf.split(combined_df))
    
    # æµ‹è¯•é›†ä¿æŒä¸å˜
    test_transform = get_multiclass_transforms(train=False)
    test_dataset = RFMiDMulticlassDataset(
        csv_path=CONFIG['test_csv'],
        image_dir=CONFIG['test_img'],
        transform=test_transform
    )
    test_loader = DataLoader(
        test_dataset, batch_size=CONFIG['batch_size']*2,
        shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True
    )
    
    logger.info(f"âœ… KæŠ˜åˆ†å‰²å®Œæˆï¼š{n_splits}æŠ˜ | æ¯æŠ˜è®­ç»ƒ/éªŒè¯çº¦ {int(len(combined_df)*0.8)}/{int(len(combined_df)*0.2)} æ ·æœ¬")
    logger.info(f"âœ… æµ‹è¯•é›†åŠ è½½ï¼š{len(test_dataset)} æ ·æœ¬")
    
    return split_indices, combined_csv_path, test_loader

# ======================== 7. å¤šåˆ†ç±»æ¨¡åž‹åˆ›å»º ========================
class SEBlock(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

def create_multiclass_model(model_name, num_classes, logger):
    """åˆ›å»ºå¤šåˆ†ç±»æ¨¡åž‹"""
    logger.info(f"ðŸ”§ åˆå§‹åŒ–å¤šåˆ†ç±»æ¨¡åž‹ï¼š{model_name}ï¼ˆè¾“å‡ºç±»åˆ«æ•°ï¼š{num_classes}ï¼‰")
    
    if model_name == 'lenet5':
        class LeNet5(nn.Module):
            def __init__(self, num_classes):
                super(LeNet5, self).__init__()
                self.features = nn.Sequential(
                    nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=2),
                    nn.Tanh(),
                    nn.AvgPool2d(kernel_size=2, stride=2),
                    nn.Conv2d(6, 16, kernel_size=5, stride=1),
                    nn.Tanh(),
                    nn.AvgPool2d(kernel_size=2, stride=2),
                    nn.Conv2d(16, 120, kernel_size=5, stride=1),
                    nn.Tanh()
                )
                self.classifier = nn.Sequential(
                    nn.Linear(120 * 56 * 56, 84),
                    nn.Tanh(),
                    nn.Linear(84, num_classes)
                )
            
            def forward(self, x):
                x = self.features(x)
                x = x.view(x.size(0), -1)
                x = self.classifier(x)
                return x
        
        model = LeNet5(num_classes=num_classes)
        logger.info("âœ… æ¨¡åž‹ç»†èŠ‚ï¼šLeNet-5ï¼ˆ1998 CNNï¼‰| æ— é¢„è®­ç»ƒ")
    
    elif model_name == 'alexnet':
        model = models.alexnet(pretrained=True)
        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)
        logger.info("âœ… æ¨¡åž‹ç»†èŠ‚ï¼šAlexNetï¼ˆ2012ï¼‰| é¢„è®­ç»ƒæƒé‡")
    
    elif model_name == 'vgg16':
        model = models.vgg16(pretrained=True)
        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)
        logger.info("âœ… æ¨¡åž‹ç»†èŠ‚ï¼šVGG16ï¼ˆ2014ï¼‰| é¢„è®­ç»ƒæƒé‡")
    
    elif model_name == 'inception_v3':
        model = models.inception_v3(pretrained=True, aux_logits=False, transform_input=True)
        model.fc = nn.Linear(model.fc.in_features, num_classes)
        logger.info("âœ… æ¨¡åž‹ç»†èŠ‚ï¼šInceptionV3ï¼ˆ2014ï¼‰| é¢„è®­ç»ƒæƒé‡")
    
    elif model_name == 'resnet50':
        model = models.resnet50(pretrained=True)
        num_ftrs = model.fc.in_features
        model.fc = nn.Linear(num_ftrs, num_classes)
        logger.info("âœ… æ¨¡åž‹ç»†èŠ‚ï¼šResNet50ï¼ˆ2015ï¼‰| é¢„è®­ç»ƒæƒé‡")
    
    elif model_name == 'densenet121':
        model = models.densenet121(pretrained=True)
        num_ftrs = model.classifier.in_features
        model.classifier = nn.Linear(num_ftrs, num_classes)
        logger.info("âœ… æ¨¡åž‹ç»†èŠ‚ï¼šDenseNet121ï¼ˆ2017ï¼‰| é¢„è®­ç»ƒæƒé‡")
    
    elif model_name == 'se_resnet50':
        class SEBottleneck(nn.Module):
            expansion = 4
            def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):
                super(SEBottleneck, self).__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * self.expansion)
                self.se = SEBlock(planes * self.expansion, reduction)
                self.relu = nn.ReLU(inplace=True)
                self.downsample = downsample
                self.stride = stride
            
            def forward(self, x):
                residual = x
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                out = self.conv2(out)
                out = self.bn2(out)
                out = self.relu(out)
                out = self.conv3(out)
                out = self.bn3(out)
                out = self.se(out)
                if self.downsample is not None:
                    residual = self.downsample(x)
                out += residual
                out = self.relu(out)
                return out
        
        from torchvision.models.resnet import ResNet
        model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)
        resnet50_pretrained = models.resnet50(pretrained=True)
        pretrained_state = resnet50_pretrained.state_dict()
        model_state = model.state_dict()
        pretrained_state = {k: v for k, v in pretrained_state.items() if k in model_state and 'se.' not in k}
        model_state.update(pretrained_state)
        model.load_state_dict(model_state)
        logger.info("âœ… æ¨¡åž‹ç»†èŠ‚ï¼šSE-ResNet50ï¼ˆ2017ï¼‰| é¢„è®­ç»ƒæƒé‡")
    
    else:
        raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ¨¡åž‹ï¼š{model_name}")
    
    return model.to(CONFIG['device'])

# ======================== 8. å¤šåˆ†ç±»è¯„ä¼°å‡½æ•° ========================
def evaluate_multiclass(model, loader, split_name, logger, criterion=None):
    """è¯„ä¼°æ¨¡åž‹æ€§èƒ½ï¼šè®¡ç®—æŸå¤±+ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š"""
    model.eval()
    all_preds = []
    all_targets = []
    total_loss = 0.0
    num_samples = 0
    
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(CONFIG['device']), labels.to(CONFIG['device'])
            outputs = model(images)
            
            if criterion is not None:
                loss = criterion(outputs, labels)
                total_loss += loss.item() * images.size(0)
                num_samples += images.size(0)
            
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())
    
    # è¿‡æ»¤æ— æ ·æœ¬ç±»åˆ«
    actual_labels = list(set(all_targets + all_preds))
    actual_labels.sort()
    CONFIG['actual_class_names'] = [CONFIG['all_class_names'][label] for label in actual_labels]
    
    avg_loss = total_loss / num_samples if num_samples > 0 else 0.0
    
    logger.info(f"\n{split_name} åˆ†ç±»æŠ¥å‘Šï¼ˆä»…æ˜¾ç¤ºæœ‰æ ·æœ¬çš„ç±»åˆ«ï¼‰| æŸå¤±: {avg_loss:.4f}")
    report = classification_report(
        all_targets, all_preds,
        labels=actual_labels,
        target_names=CONFIG['actual_class_names'],
        digits=2,
        zero_division=0
    )
    logger.info(report)
    print(report)
    
    report_dict = classification_report(
        all_targets, all_preds,
        labels=actual_labels,
        target_names=CONFIG['actual_class_names'],
        output_dict=True,
        zero_division=0
    )
    report_dict['loss'] = avg_loss
    return report_dict

# ======================== 9. è®­ç»ƒå‡½æ•° ========================
def train_one_epoch(model, loader, criterion, optimizer, epoch, logger):
    model.train()
    total_loss = 0.0
    
    for batch_idx, (images, labels) in enumerate(loader):
        images, labels = images.to(CONFIG['device']), labels.to(CONFIG['device'])
        
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * images.size(0)
        
        if (batch_idx + 1) % 10 == 0:
            log_msg = f"Epoch [{epoch+1}/{CONFIG['epochs']}] | Batch [{batch_idx+1}/{len(loader)}] | Loss: {loss.item():.4f}"
            logger.info(log_msg)
            print(log_msg)
    
    avg_loss = total_loss / len(loader.dataset)
    log_msg = f"Epoch [{epoch+1}] è®­ç»ƒæŸå¤±ï¼š{avg_loss:.4f}"
    logger.info(log_msg)
    print(log_msg)
    return avg_loss

# ======================== 10. KæŠ˜è®­ç»ƒæµç¨‹ï¼ˆæ ¸å¿ƒæ”¹é€ ï¼‰ ========================
def train_kfold_multiclass_model(model_name):
    """å•ä¸ªæ¨¡åž‹çš„5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒæµç¨‹"""
    print(f"\n{'='*80}")
    print(f"å¼€å§‹è®­ç»ƒå¤šåˆ†ç±»æ¨¡åž‹ï¼š{model_name}ï¼ˆ{CONFIG['n_splits']}æŠ˜äº¤å‰éªŒè¯ï¼‰")
    print(f"{'='*80}")
    
    # 1. åˆ›å»ºæ±‡æ€»æ—¥å¿—ï¼ˆè·¨æŠ˜æ€»ç»“ï¼‰
    summary_logger, summary_log_file = setup_logger(model_name, fold_idx=None)
    summary_logger.info(f"{'='*70}")
    summary_logger.info(f"æ¨¡åž‹ {model_name} å¼€å§‹{CONFIG['n_splits']}æŠ˜äº¤å‰éªŒè¯")
    summary_logger.info(f"{'='*70}")
    
    # 2. æå–æ‰€æœ‰ç±»åˆ«
    extract_all_classes_from_col3(CONFIG['train_csv'], summary_logger)
    
    # 3. åˆ›å»ºKæŠ˜åˆ†å‰²å™¨å’Œæµ‹è¯•é›†
    split_indices, combined_csv_path, test_loader = create_kfold_dataloaders(summary_logger, CONFIG['n_splits'])
    
    # 4. è·¨æŠ˜ç»“æžœè®°å½•
    fold_results = []
    best_models = []
    
    # 5. å¾ªçŽ¯æ¯ä¸€æŠ˜
    for fold_idx, (train_idx, val_idx) in enumerate(split_indices):
        print(f"\n{'='*60}")
        print(f"æ¨¡åž‹ {model_name} | ç¬¬ {fold_idx+1}/{CONFIG['n_splits']} æŠ˜è®­ç»ƒ")
        print(f"{'='*60}")
        
        # ä¸ºæ¯æŠ˜åˆ›å»ºç‹¬ç«‹æ—¥å¿—
        logger, log_file = setup_logger(model_name, fold_idx)
        logger.info(f"{'='*60}")
        logger.info(f"æ¨¡åž‹ {model_name} | ç¬¬ {fold_idx+1}/{CONFIG['n_splits']} æŠ˜")
        logger.info(f"è®­ç»ƒæ ·æœ¬æ•°ï¼š{len(train_idx)} | éªŒè¯æ ·æœ¬æ•°ï¼š{len(val_idx)}")
        logger.info(f"{'='*60}\n")
        
        try:
            # 6. åˆ›å»ºè¯¥æŠ˜çš„æ•°æ®åŠ è½½å™¨
            train_transform = get_multiclass_transforms(train=True)
            val_transform = get_multiclass_transforms(train=False)
            
            train_dataset = RFMiDMulticlassDataset(
                csv_path=combined_csv_path,
                image_dir=None,  # è‡ªåŠ¨æ‰«æè®­ç»ƒå’ŒéªŒè¯ç›®å½•
                transform=train_transform,
                subset_idx=train_idx
            )
            val_dataset = RFMiDMulticlassDataset(
                csv_path=combined_csv_path,
                image_dir=None,
                transform=val_transform,
                subset_idx=val_idx
            )
            
            train_loader = DataLoader(
                train_dataset, batch_size=CONFIG['batch_size'],
                shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=True
            )
            val_loader = DataLoader(
                val_dataset, batch_size=CONFIG['batch_size']*2,
                shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True
            )
            
            logger.info(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼šè®­ç»ƒé›† {len(train_dataset)} | éªŒè¯é›† {len(val_dataset)}")
            
            # 7. åˆå§‹åŒ–è¯¥æŠ˜æ¨¡åž‹ï¼ˆæ¯æŠ˜é‡æ–°åˆå§‹åŒ–ï¼‰
            num_classes = len(CONFIG['all_class_names'])
            model = create_multiclass_model(model_name, num_classes, logger)
            
            # 8. åˆå§‹åŒ–æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.AdamW(
                model.parameters(),
                lr=CONFIG['lr'],
                weight_decay=CONFIG['weight_decay']
            )
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)
            
            # 9. æ¯æŠ˜è®°å½•CSVï¼ˆæ–°å¢žï¼‰
            records_dir = os.path.join(CONFIG['output_dir'], 'training_records', model_name)
            os.makedirs(records_dir, exist_ok=True)
            fold_record_csv = os.path.join(records_dir, f'{model_name}_fold{fold_idx+1}_losses.csv')
            with open(fold_record_csv, 'w', newline='', encoding='utf-8') as f:
                csv.writer(f).writerow(['Epoch', 'Train_Loss', 'Val_Loss', 'Val_Macro_F1'])
            
            # 10. è®­ç»ƒè®°å½•
            best_val_macro_f1 = 0.0
            model_save_dir = os.path.join(CONFIG['output_dir'], 'best_models', model_name)
            os.makedirs(model_save_dir, exist_ok=True)
            best_model_path = os.path.join(model_save_dir, f'{model_name}_fold{fold_idx+1}_best.pth')
            
            # 11. è®­ç»ƒå¾ªçŽ¯
            for epoch in range(CONFIG['epochs']):
                logger.info(f"\n{'='*40} Epoch {epoch+1}/{CONFIG['epochs']} {'='*40}")
                print(f"\n{'='*40} Epoch {epoch+1}/{CONFIG['epochs']} {'='*40}")
                
                train_loss = train_one_epoch(model, train_loader, criterion, optimizer, epoch, logger)
                val_report = evaluate_multiclass(model, val_loader, "Validation", logger, criterion)
                val_loss = val_report['loss']
                val_macro_f1 = val_report['macro avg']['f1-score']
                
                # è®°å½•åˆ°CSV
                with open(fold_record_csv, 'a', newline='', encoding='utf-8') as f:
                    csv.writer(f).writerow([epoch+1, f"{train_loss:.4f}", f"{val_loss:.4f}", f"{val_macro_f1:.4f}"])
                
                scheduler.step(val_loss)
                
                if val_macro_f1 > best_val_macro_f1:
                    best_val_macro_f1 = val_macro_f1
                    torch.save({
                        'fold': fold_idx+1,
                        'epoch': epoch,
                        'model_name': model_name,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'best_val_macro_f1': best_val_macro_f1,
                        'val_metrics': val_report,
                        'config': CONFIG
                    }, best_model_path)
                    logger.info(f"âœ… ä¿å­˜æœ€ä½³æ¨¡åž‹ï¼ˆF1: { best_val_macro_f1:.4f}ï¼‰åˆ°ï¼š{best_model_path}")
                    print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡åž‹ï¼ˆF1: {best_val_macro_f1:.4f}ï¼‰åˆ°ï¼š{best_model_path}")
            
            # 12. è®°å½•è¯¥æŠ˜ç»“æžœ
            fold_results.append({
                'fold': fold_idx+1,
                'best_val_macro_f1': best_val_macro_f1,
                'val_report': val_report,
                'model_path': best_model_path
            })
            best_models.append(best_model_path)
            
            logger.info(f"\n{'='*60}")
            logger.info(f"ç¬¬{fold_idx+1}æŠ˜è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å®å¹³å‡F1ï¼š{best_val_macro_f1:.4f}")
            logger.info(f"{'='*60}\n")
            
        except Exception as e:
            logger.error(f"âŒ ç¬¬{fold_idx+1}æŠ˜è®­ç»ƒå¤±è´¥ï¼š{str(e)}", exc_info=True)
            continue
    
    # 13. è®¡ç®—äº¤å‰éªŒè¯å¹³å‡ç»“æžœ
    if len(fold_results) > 0:
        avg_val_macro_f1 = np.mean([r['best_val_macro_f1'] for r in fold_results])
        std_val_macro_f1 = np.std([r['best_val_macro_f1'] for r in fold_results])
    else:
        avg_val_macro_f1 = std_val_macro_f1 = float('nan')
    
    # 14. ä¿å­˜äº¤å‰éªŒè¯ç»“æžœåˆ°æ±‡æ€»æ—¥å¿—
    summary_logger.info(f"\n{'='*70}")
    summary_logger.info(f"æ¨¡åž‹ {model_name} {CONFIG['n_splits']}æŠ˜äº¤å‰éªŒè¯å®Œæˆ")
    summary_logger.info(f"{'='*70}")
    for result in fold_results:
        summary_logger.info(f"ç¬¬{result['fold']}æŠ˜ | éªŒè¯å®å¹³å‡F1: {result['best_val_macro_f1']:.4f} | æ¨¡åž‹: {result['model_path']}")
    summary_logger.info(f"\n{'='*70}")
    summary_logger.info(f"å¹³å‡éªŒè¯å®å¹³å‡F1ï¼š{avg_val_macro_f1:.4f} Â± {std_val_macro_f1:.4f}")
    summary_logger.info(f"{'='*70}")
    
    print(f"\nâœ… {model_name} 5æŠ˜äº¤å‰éªŒè¯å®Œæˆï¼å¹³å‡éªŒè¯å®å¹³å‡F1ï¼š{avg_val_macro_f1:.4f} Â± {std_val_macro_f1:.4f}")
    
    # 15. æµ‹è¯•é›†è¯„ä¼°ï¼ˆä½¿ç”¨ç¬¬1æŠ˜æ¨¡åž‹ï¼‰
    if len(best_models) > 0:
        print(f"\n{'='*70}")
        print(f"ä½¿ç”¨ç¬¬1æŠ˜æœ€ä½³æ¨¡åž‹è¿›è¡Œæµ‹è¯•é›†è¯„ä¼°")
        print(f"{'='*70}")
        
        logger, _ = setup_logger(model_name, 0)
        checkpoint = torch.load(best_models[0])
        model = create_multiclass_model(model_name, num_classes, logger)
        model.load_state_dict(checkpoint['model_state_dict'])
        
        criterion = nn.CrossEntropyLoss()
        final_test_report = evaluate_multiclass(model, test_loader, "Final Test", logger, criterion)
        
        summary_logger.info(f"\n{'='*70}")
        summary_logger.info(f"æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ç»“æžœï¼ˆä½¿ç”¨ç¬¬1æŠ˜æ¨¡åž‹ï¼‰")
        summary_logger.info(f"æŸå¤±: {final_test_report['loss']:.4f} | å®å¹³å‡F1: {final_test_report['macro avg']['f1-score']:.4f}")
        summary_logger.info(f"{'='*70}")
        
        print(f"æµ‹è¯•é›†ç»“æžœï¼šæŸå¤±={final_test_report['loss']:.4f} å®å¹³å‡F1={final_test_report['macro avg']['f1-score']:.4f}")
    
    return fold_results, summary_log_file

# ======================== 11. å¤šæ¨¡åž‹æ‰¹é‡è®­ç»ƒï¼ˆä¸»æµç¨‹ï¼‰ ========================
def main():
    print(f"{'='*70}")
    print(f"RFMiDå¤šåˆ†ç±»å¤šæ¨¡åž‹5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ")
    print(f"è®­ç»ƒæ¨¡åž‹åˆ—è¡¨ï¼š{CONFIG['models_to_train']}")
    print(f"è®¾å¤‡ï¼š{CONFIG['device']} | è¾“å‡ºç›®å½•ï¼š{CONFIG['output_dir']}")
    print(f"{'='*70}\n")
    
    all_models_results = {}
    
    for model_name in CONFIG['models_to_train']:
        print(f"\n{'='*80}")
        print(f"æ­£åœ¨è®­ç»ƒå¤šåˆ†ç±»æ¨¡åž‹ï¼š{model_name}")
        print(f"{'='*80}")
        
        fold_results, summary_log = train_kfold_multiclass_model(model_name)
        all_models_results[model_name] = {
            'fold_results': fold_results,
            'summary_log': summary_log
        }
        
        print(f"\nâœ… {model_name} è®­ç»ƒå®Œæˆï¼æ—¥å¿—å·²ä¿å­˜åˆ°ï¼š{summary_log}")
        print(f"{'='*80}\n")
    
    print(f"\n{'='*70}")
    print(f"æ‰€æœ‰æ¨¡åž‹5æŠ˜äº¤å‰éªŒè¯å®Œæˆï¼")
    print(f"ç»“æžœæ±‡æ€»ï¼š")
    print(f" - è¯¦ç»†æ—¥å¿—ï¼š{os.path.join(CONFIG['output_dir'], 'logs')}")
    print(f" - æœ€ä½³æ¨¡åž‹ï¼š{os.path.join(CONFIG['output_dir'], 'best_models')}")
    print(f" - æŸå¤±è®°å½•ï¼š{os.path.join(CONFIG['output_dir'], 'training_records')}")
    print(f" - å„æ¨¡åž‹å¹³å‡æ€§èƒ½ï¼š")
    for model_name, results in all_models_results.items():
        fold_results = results['fold_results']
        if len(fold_results) > 0:
            avg_f1 = np.mean([r['best_val_macro_f1'] for r in fold_results])
            std_f1 = np.std([r['best_val_macro_f1'] for r in fold_results])
            print(f"   * {model_name}: {avg_f1:.4f} Â± {std_f1:.4f}")
        else:
            print(f"   * {model_name}: è®­ç»ƒå¤±è´¥")
    print(f"{'='*70}")

if __name__ == "__main__":
    main()