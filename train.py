import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import pandas as pd
import os
from PIL import Image
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score
import warnings
warnings.filterwarnings('ignore')
import logging
from datetime import datetime
import efficientnet_pytorch  # éœ€é¢å¤–å®‰è£…ï¼špip install efficientnet-pytorch

# ======================== 1. æ ¸å¿ƒé…ç½®ï¼ˆä¿®æ”¹è¿™é‡Œé€‚é…ä½ çš„ç¯å¢ƒï¼‰ ========================
CONFIG = {
    # è·¯å¾„é…ç½®
    'train_csv': r'E:\eyes__disease\dataset\Training_Set\RFMiD_Training_Labels.csv',
    'train_img': r'E:\eyes__disease\dataset\Training_Set\Training',
    'val_csv': r'E:\eyes__disease\dataset\Evaluation_Set\RFMiD_Validation_Labels.csv',
    'val_img': r'E:\eyes__disease\dataset\Evaluation_Set\Validation',
    'test_csv': r'E:\eyes__disease\dataset\Test_Set\RFMiD_Testing_Labels.csv',
    'test_img': r'E:\eyes__disease\dataset\Test_Set\Test',
    
    # è®­ç»ƒå‚æ•°
    'img_size': (224, 224),    # å›¾ç‰‡å°ºå¯¸ï¼ˆInceptionV3å»ºè®®æ”¹ä¸º299Ã—299ï¼Œå…¶ä»–æ¨¡å‹å…¼å®¹224ï¼‰
    'batch_size': 16,          # é€‚é…å¤šæ¨¡å‹ï¼ˆæ˜¾å­˜ä¸è¶³å¯æ”¹8ï¼›VGG16/InceptionV3å»ºè®®8ï¼‰
    'num_workers': 12,          # CPUæ ¸å¿ƒæ•°
    'img_ext': '.png',          # å›¾ç‰‡æ‰©å±•å
    'epochs': 30,              # è®­ç»ƒè½®æ¬¡
    'lr': 1e-4,                # å­¦ä¹ ç‡ï¼ˆInceptionV3/SE-ResNet50å»ºè®®5e-5ï¼‰
    'weight_decay': 1e-5,       # æƒé‡è¡°å‡ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',  # è‡ªåŠ¨æ£€æµ‹GPU/CPU
    'output_dir': 'model_comparison_results',  # æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜æ ¹ç›®å½•
    'models_to_train': [        # è¦æ¯”å¯¹çš„ç»å…¸æ¨¡å‹åˆ—è¡¨ï¼ˆå¯å¢åˆ ï¼‰

        #'resnet50', # ResNet 2015
        #'densenet121', # DenseNet 2017
        #'vgg16'
        'alexnet'
    ]
}

# ======================== 2. æ—¥å¿—é…ç½®ï¼ˆä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºç‹¬ç«‹æ—¥å¿—ï¼‰ ========================
def setup_logger(model_name):
    """åˆ›å»ºæ¨¡å‹ä¸“å±æ—¥å¿—å™¨ï¼Œä¿å­˜è®­ç»ƒè®°å½•"""
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(CONFIG['output_dir'], exist_ok=True)
    log_dir = os.path.join(CONFIG['output_dir'], 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    # æ—¥å¿—æ–‡ä»¶åï¼šæ¨¡å‹å_æ—¶é—´.log
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(log_dir, f'{model_name}_train_{timestamp}.log')
    
    # é…ç½®æ—¥å¿—æ ¼å¼
    logger = logging.getLogger(model_name)
    logger.setLevel(logging.INFO)
    logger.propagate = False  # é¿å…é‡å¤è¾“å‡º
    
    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ï¼‰
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    
    # æ§åˆ¶å°å¤„ç†å™¨ï¼ˆåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼‰
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger, log_file

# ======================== 3. è‡ªå®šä¹‰äºŒåˆ†ç±»æ•°æ®é›†ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰ ========================
class RFMiDBinaryDataset(Dataset):
    def __init__(self, csv_path, image_dir, transform=None):
        self.csv_df = pd.read_csv(csv_path)
        self.image_dir = image_dir
        self.transform = transform
        
        # äºŒåˆ†ç±»æ ‡ç­¾ï¼šDisease_Riskï¼ˆ0=æ­£å¸¸ï¼Œ1=ç–¾ç—…ï¼‰
        self.labels = self.csv_df['Disease_Risk'].values
        self.img_ids = self.csv_df['ID'].values
        
        # å›¾ç‰‡è·¯å¾„æ˜ å°„
        self.img_path_dict = {
            int(os.path.splitext(f)[0]): os.path.join(image_dir, f)
            for f in os.listdir(image_dir) if f.endswith(CONFIG['img_ext'])
        }

    def __len__(self):
        return len(self.csv_df)

    def __getitem__(self, idx):
        img_id = self.img_ids[idx]
        img_path = self.img_path_dict[img_id]
        
        # è¯»å–å›¾ç‰‡ï¼ˆè½¬ä¸ºRGBï¼Œå¤„ç†æŸåå›¾ç‰‡ï¼‰
        try:
            image = Image.open(img_path).convert('RGB')
        except:
            image = Image.fromarray(np.random.randint(0, 255, size=CONFIG['img_size'] + (3,), dtype=np.uint8))
        
        # åº”ç”¨é¢„å¤„ç†/å¢å¼º
        if self.transform:
            image = self.transform(image)
        
        # äºŒåˆ†ç±»æ ‡ç­¾ï¼ˆfloat32é€‚é…BCELossï¼‰
        label = torch.tensor(self.labels[idx], dtype=torch.float32)
        
        return image, label

# ======================== 4. æ•°æ®é¢„å¤„ç†/å¢å¼ºï¼ˆä¿æŒåŸé€»è¾‘ï¼‰ ========================
def get_binary_transforms(train=True):
    transform_list = [
        transforms.Resize(CONFIG['img_size']),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNetå½’ä¸€åŒ–
                           std=[0.229, 0.224, 0.225])
    ]
    
    if train:
        # éšæœºæ·»åŠ å¢å¼ºï¼ˆæ¯æ¬¡è®­ç»ƒéšæœºè§¦å‘ï¼‰
        augmentations = [
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
        ]
        np.random.shuffle(augmentations)
        selected_augs = augmentations[:np.random.randint(1, 3)]
        transform_list = selected_augs + transform_list
    
    return transforms.Compose(transform_list)

# ======================== 5. åˆ›å»ºDataLoaderï¼ˆä¿æŒåŸé€»è¾‘ï¼‰ ========================
def create_binary_dataloaders(logger):
    train_transform = get_binary_transforms(train=True)
    val_test_transform = get_binary_transforms(train=False)
    
    # æ•°æ®é›†å®ä¾‹
    train_dataset = RFMiDBinaryDataset(
        csv_path=CONFIG['train_csv'],
        image_dir=CONFIG['train_img'],
        transform=train_transform
    )
    val_dataset = RFMiDBinaryDataset(
        csv_path=CONFIG['val_csv'],
        image_dir=CONFIG['val_img'],
        transform=val_test_transform
    )
    test_dataset = RFMiDBinaryDataset(
        csv_path=CONFIG['test_csv'],
        image_dir=CONFIG['test_img'],
        transform=val_test_transform
    )
    
    # DataLoader
    train_loader = DataLoader(
        train_dataset, batch_size=CONFIG['batch_size'],
        shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=CONFIG['batch_size']*2,
        shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset, batch_size=CONFIG['batch_size']*2,
        shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True
    )
    
    logger.info(f"âœ… DataLoaderåˆ›å»ºå®Œæˆï¼š")
    logger.info(f" - è®­ç»ƒé›†ï¼š{len(train_dataset)}æ ·æœ¬ | {len(train_loader)}æ‰¹æ¬¡")
    logger.info(f" - éªŒè¯é›†ï¼š{len(val_dataset)}æ ·æœ¬ | {len(val_loader)}æ‰¹æ¬¡")
    logger.info(f" - æµ‹è¯•é›†ï¼š{len(test_dataset)}æ ·æœ¬ | {len(test_loader)}æ‰¹æ¬¡")
    
    return train_loader, val_loader, test_loader

# ======================== 6. å¤šä¸ªç»å…¸æ¨¡å‹åˆ›å»ºå‡½æ•°ï¼ˆæ ¸å¿ƒæ–°å¢æ‰€æœ‰ç›®æ ‡æ¨¡å‹ï¼‰ ========================
## è¾…åŠ©æ¨¡å—ï¼šSEæ¨¡å—ï¼ˆç”¨äºSE-Net 2017ï¼‰
class SEBlock(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # æŒ¤å‹ï¼šå…¨å±€å¹³å‡æ± åŒ–
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel),
            nn.Sigmoid()  # æ¿€åŠ±ï¼šè¾“å‡ºæ³¨æ„åŠ›æƒé‡
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)  # (b,c,h,w) â†’ (b,c)
        y = self.fc(y).view(b, c, 1, 1)  # (b,c) â†’ (b,c,1,1)
        return x * y.expand_as(x)  # æ³¨æ„åŠ›æƒé‡ä¹˜åŸç‰¹å¾å›¾

def create_model(model_name, logger):
    """æ ¹æ®æ¨¡å‹ååˆ›å»ºå¯¹åº”çš„äºŒåˆ†ç±»æ¨¡å‹ï¼ˆæ–°å¢æ‰€æœ‰ç›®æ ‡æ¨¡å‹ï¼‰"""
    logger.info(f"ğŸ”§ åˆå§‹åŒ–æ¨¡å‹ï¼š{model_name}")
    num_classes = 1  # äºŒåˆ†ç±»è¾“å‡º
    
    if model_name == 'lenet5':
        # CNN 1998ï¼ˆLeNet-5ï¼‰ï¼šæœ€æ—©CNNï¼Œå¥ å®šåŸºç¡€
        class LeNet5(nn.Module):
            def __init__(self, num_classes=1):
                super(LeNet5, self).__init__()
                # ç‰¹å¾æå–ï¼šå·ç§¯+æ± åŒ–ï¼ˆç»å…¸LeNetç»“æ„ï¼‰
                self.features = nn.Sequential(
                    nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=2),  # RGBè¾“å…¥â†’6é€šé“
                    nn.Tanh(),  # åŸå§‹ç”¨sigmoidï¼ŒTanhæ›´ç¨³å®š
                    nn.AvgPool2d(kernel_size=2, stride=2),  # å°ºå¯¸/2
                    nn.Conv2d(6, 16, kernel_size=5, stride=1),
                    nn.Tanh(),
                    nn.AvgPool2d(kernel_size=2, stride=2),  # å°ºå¯¸/2
                    nn.Conv2d(16, 120, kernel_size=5, stride=1),
                    nn.Tanh()
                )
                # åˆ†ç±»å±‚ï¼šé€‚é…224Ã—224è¾“å…¥ï¼ˆ2æ¬¡æ± åŒ–åå°ºå¯¸ï¼š224â†’112â†’56ï¼‰
                self.classifier = nn.Sequential(
                    nn.Linear(120 * 56 * 56, 84),  # 120Ã—56Ã—56 = 376320
                    nn.Tanh(),
                    nn.Linear(84, num_classes)
                )
            
            def forward(self, x):
                x = self.features(x)
                x = x.view(x.size(0), -1)  # å±•å¹³
                x = self.classifier(x)
                return x
        
        model = LeNet5(num_classes=num_classes)
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šLeNet-5ï¼ˆ1998ç»å…¸CNNï¼‰| æ— é¢„è®­ç»ƒ | å·ç§¯Ã—3+æ± åŒ–Ã—2+å…¨è¿æ¥Ã—2")
    
    elif model_name == 'alexnet':
        # AlexNet 2012ï¼šReLUã€Dropoutã€GPUè®­ç»ƒ
        model = models.alexnet(pretrained=True)
        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)  # 1000ç±»â†’1ç±»
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šAlexNetï¼ˆ2012ï¼‰| é¢„è®­ç»ƒæƒé‡ | ReLU+Dropout+GPUé€‚é…")
    
    elif model_name == 'vgg16':
        # VGGNet 2014ï¼š3Ã—3å°å·ç§¯å †å ï¼Œç»“æ„ç®€æ´
        model = models.vgg16(pretrained=True)
        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)  # 1000ç±»â†’1ç±»
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šVGG16ï¼ˆ2014ï¼‰| é¢„è®­ç»ƒæƒé‡ | 3Ã—3å°å·ç§¯å †å +å…¨è¿æ¥å±‚")
    
    elif model_name == 'inception_v3':
        # GoogLeNet/Inception 2014ï¼šInceptionæ¨¡å—ï¼Œå¤šå°ºåº¦ç‰¹å¾
        # æ³¨æ„ï¼šInceptionV3é»˜è®¤è¾“å…¥å°ºå¯¸â‰¥299ï¼Œè‹¥ç”¨224éœ€è®¾ç½®transform_input=True
        model = models.inception_v3(pretrained=True, aux_logits=False, transform_input=True)
        model.fc = nn.Linear(model.fc.in_features, num_classes)  # 1000ç±»â†’1ç±»
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šInceptionV3ï¼ˆ2014ï¼‰| é¢„è®­ç»ƒæƒé‡ | Inceptionæ¨¡å—+å¤šå°ºåº¦ç‰¹å¾èåˆ")
        logger.warning("âš ï¸  å»ºè®®ï¼šInceptionV3æœ€ä½³è¾“å…¥å°ºå¯¸299Ã—299ï¼Œå¯ä¿®æ”¹CONFIG['img_size']æå‡æ€§èƒ½")
    
    elif model_name == 'resnet50':
        # ResNet 2015ï¼šæ®‹å·®è¿æ¥ï¼Œè§£å†³æ¢¯åº¦æ¶ˆå¤±
        model = models.resnet50(pretrained=True)
        num_ftrs = model.fc.in_features
        model.fc = nn.Linear(num_ftrs, num_classes)  # 1000ç±»â†’1ç±»
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šResNet50ï¼ˆ2015ï¼‰| é¢„è®­ç»ƒæƒé‡ | æ®‹å·®è¿æ¥+æ·±å±‚ç½‘ç»œ")
    
    elif model_name == 'densenet121':
        # DenseNet 2017ï¼šå¯†é›†è¿æ¥ï¼Œç‰¹å¾å¤ç”¨
        model = models.densenet121(pretrained=True)
        num_ftrs = model.classifier.in_features
        model.classifier = nn.Linear(num_ftrs, num_classes)  # 1000ç±»â†’1ç±»
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šDenseNet121ï¼ˆ2017ï¼‰| é¢„è®­ç»ƒæƒé‡ | å¯†é›†è¿æ¥+ç‰¹å¾å¤ç”¨")
    
    elif model_name == 'se_resnet50':
        # SE-Net 2017ï¼šé€šé“æ³¨æ„åŠ›æœºåˆ¶ï¼Œå³æ’å³ç”¨
        class SEBottleneck(nn.Module):
            expansion = 4
            def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):
                super(SEBottleneck, self).__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * self.expansion)
                self.se = SEBlock(planes * self.expansion, reduction)  # æ’å…¥SEæ¨¡å—
                self.relu = nn.ReLU(inplace=True)
                self.downsample = downsample
                self.stride = stride
            
            def forward(self, x):
                residual = x
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                out = self.conv2(out)
                out = self.bn2(out)
                out = self.relu(out)
                out = self.conv3(out)
                out = self.bn3(out)
                out = self.se(out)  # åº”ç”¨é€šé“æ³¨æ„åŠ›
                if self.downsample is not None:
                    residual = self.downsample(x)
                out += residual  # æ®‹å·®è¿æ¥
                out = self.relu(out)
                return out
        
        # æ„å»ºSE-ResNet50
        from torchvision.models.resnet import ResNet
        model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)
        # åŠ è½½ResNet50é¢„è®­ç»ƒæƒé‡ï¼ˆè¿‡æ»¤SEæ¨¡å—çš„æƒé‡ï¼‰
        resnet50_pretrained = models.resnet50(pretrained=True)
        pretrained_state = resnet50_pretrained.state_dict()
        model_state = model.state_dict()
        pretrained_state = {k: v for k, v in pretrained_state.items() if k in model_state and 'se.' not in k}
        model_state.update(pretrained_state)
        model.load_state_dict(model_state)
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šSE-ResNet50ï¼ˆ2017ï¼‰| é¢„è®­ç»ƒæƒé‡ | é€šé“æ³¨æ„åŠ›+æ®‹å·®è¿æ¥")
    
    else:
        raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ï¼š{model_name}ï¼ˆè¯·ä»CONFIG['models_to_train']ä¸­é€‰æ‹©ï¼‰")
    
    # ç§»åˆ°è®¾å¤‡
    model = model.to(CONFIG['device'])
    logger.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè®¾å¤‡ï¼š{CONFIG['device']}")
    return model

# ======================== 7. è¯„ä»·æŒ‡æ ‡è®¡ç®—ï¼ˆä¿æŒåŸé€»è¾‘ï¼‰ ========================
def calculate_metrics(preds, targets):
    preds_binary = (preds > 0.5).float().cpu().numpy()
    targets_np = targets.cpu().numpy()
    
    precision = precision_score(targets_np, preds_binary, zero_division=0)
    recall = recall_score(targets_np, preds_binary, zero_division=0)
    f1 = f1_score(targets_np, preds_binary, zero_division=0)
    
    return {
        'Precision': round(precision, 4),
        'Recall': round(recall, 4),
        'F1': round(f1, 4)
    }

# ======================== 8. è®­ç»ƒ/éªŒè¯/æµ‹è¯•å‡½æ•°ï¼ˆé›†æˆæ—¥å¿—ï¼‰ ========================
def train_one_epoch(model, loader, criterion, optimizer, epoch, logger):
    model.train()
    total_loss = 0.0
    all_preds = []
    all_targets = []
    
    for batch_idx, (images, labels) in enumerate(loader):
        images, labels = images.to(CONFIG['device']), labels.to(CONFIG['device'])
        
        outputs = model(images)
        loss = criterion(outputs.squeeze(), labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * images.size(0)
        all_preds.extend(torch.sigmoid(outputs).detach())
        all_targets.extend(labels.detach())
        
        # æ¯10ä¸ªbatchæ‰“å°æ—¥å¿—
        if (batch_idx + 1) % 10 == 0:
            logger.info(f"Epoch [{epoch+1}/{CONFIG['epochs']}] | Batch [{batch_idx+1}/{len(loader)}] | Loss: {loss.item():.4f}")
    
    avg_loss = total_loss / len(loader.dataset)
    metrics = calculate_metrics(torch.stack(all_preds), torch.stack(all_targets))
    
    logger.info(f"Epoch [{epoch+1}] è®­ç»ƒç»“æœ | Loss: {avg_loss:.4f} | Precision: {metrics['Precision']} | Recall: {metrics['Recall']} | F1: {metrics['F1']}")
    return avg_loss, metrics

def evaluate(model, loader, criterion, split_name, logger):
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(CONFIG['device']), labels.to(CONFIG['device'])
            
            outputs = model(images)
            loss = criterion(outputs.squeeze(), labels)
            
            total_loss += loss.item() * images.size(0)
            all_preds.extend(torch.sigmoid(outputs).detach())
            all_targets.extend(labels.detach())
    
    avg_loss = total_loss / len(loader.dataset)
    metrics = calculate_metrics(torch.stack(all_preds), torch.stack(all_targets))
    
    logger.info(f"\n{split_name} ç»“æœ | Loss: {avg_loss:.4f} | Precision: {metrics['Precision']} | Recall: {metrics['Recall']} | F1: {metrics['F1']}\n")
    return avg_loss, metrics

# ======================== 9. å•ä¸ªæ¨¡å‹è®­ç»ƒæµç¨‹ï¼ˆé›†æˆæ—¥å¿—å’Œæ¨¡å‹ä¿å­˜ï¼‰ ========================
def train_single_model(model_name):
    # 1. åˆå§‹åŒ–æ—¥å¿—
    logger, log_file = setup_logger(model_name)
    logger.info(f"{'='*60}")
    logger.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼š{model_name}")
    logger.info(f"è®­ç»ƒé…ç½®ï¼š{CONFIG}")
    logger.info(f"{'='*60}\n")
    
    try:
        # 2. åˆ›å»ºDataLoader
        train_loader, val_loader, test_loader = create_binary_dataloaders(logger)
        
        # 3. åˆ›å»ºæ¨¡å‹
        model = create_model(model_name, logger)
        
        # 4. åˆå§‹åŒ–æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨
        criterion = nn.BCEWithLogitsLoss()
        optimizer = optim.AdamW(
            model.parameters(),
            lr=CONFIG['lr'],
            weight_decay=CONFIG['weight_decay']
        )
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)
        
        # 5. è®­ç»ƒè®°å½•
        best_val_f1 = 0.0
        model_save_dir = os.path.join(CONFIG['output_dir'], 'best_models')
        os.makedirs(model_save_dir, exist_ok=True)
        best_model_path = os.path.join(model_save_dir, f'{model_name}_best.pth')
        
        # 6. è®­ç»ƒå¾ªç¯
        for epoch in range(CONFIG['epochs']):
            logger.info(f"\n{'='*40} Epoch {epoch+1}/{CONFIG['epochs']} {'='*40}")
            
            # è®­ç»ƒ
            train_loss, train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, epoch, logger)
            
            # éªŒè¯
            val_loss, val_metrics = evaluate(model, val_loader, criterion, "Validation", logger)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›†F1ï¼‰
            if val_metrics['F1'] > best_val_f1:
                best_val_f1 = val_metrics['F1']
                torch.save({
                    'epoch': epoch,
                    'model_name': model_name,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_f1': best_val_f1,
                    'val_metrics': val_metrics,
                    'config': CONFIG
                }, best_model_path)
                logger.info(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆF1: {best_val_f1:.4f}ï¼‰åˆ°ï¼š{best_model_path}")
        
        # 7. æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ï¼ˆåŠ è½½æœ€ä½³æ¨¡å‹ï¼‰
        logger.info(f"\n{'='*60}")
        logger.info(f"æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ï¼ˆåŠ è½½æœ€ä½³æ¨¡å‹ï¼‰")
        logger.info(f"{'='*60}")
        
        checkpoint = torch.load(best_model_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        test_loss, test_metrics = evaluate(model, test_loader, criterion, "Test", logger)
        
        # 8. è®­ç»ƒæ€»ç»“
        logger.info(f"\n{'='*60}")
        logger.info(f"{model_name} è®­ç»ƒæ€»ç»“ï¼š")
        logger.info(f" - æœ€ä½³éªŒè¯F1ï¼š{best_val_f1:.4f}")
        logger.info(f" - æµ‹è¯•é›†Precisionï¼š{test_metrics['Precision']}")
        logger.info(f" - æµ‹è¯•é›†Recallï¼š{test_metrics['Recall']}")
        logger.info(f" - æµ‹è¯•é›†F1ï¼š{test_metrics['F1']}")
        logger.info(f" - æœ€ä½³æ¨¡å‹è·¯å¾„ï¼š{best_model_path}")
        logger.info(f" - æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼š{log_file}")
        logger.info(f"{'='*60}")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™ï¼š{str(e)}", exc_info=True)
        raise

# ======================== 10. å¤šæ¨¡å‹æ‰¹é‡è®­ç»ƒï¼ˆä¸»æµç¨‹ï¼‰ ========================
def main():
    print(f"{'='*70}")
    print(f"å¼€å§‹å¤šç»å…¸æ¨¡å‹æ¯”å¯¹è®­ç»ƒï¼ˆäºŒåˆ†ç±»ï¼šNormal vs Diseaseï¼‰")
    print(f"è®­ç»ƒæ¨¡å‹åˆ—è¡¨ï¼š{CONFIG['models_to_train']}")
    print(f"è®¾å¤‡ï¼š{CONFIG['device']} | è¾“å‡ºç›®å½•ï¼š{CONFIG['output_dir']}")
    print(f"{'='*70}\n")
    
    # å¾ªç¯è®­ç»ƒæ¯ä¸ªæ¨¡å‹
    for model_name in CONFIG['models_to_train']:
        print(f"\n{'='*80}")
        print(f"æ­£åœ¨è®­ç»ƒæ¨¡å‹ï¼š{model_name}")
        print(f"{'='*80}")
        
        train_single_model(model_name)
        
        print(f"\nâœ… {model_name} è®­ç»ƒå®Œæˆï¼æ—¥å¿—å’Œæ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{CONFIG['output_dir']}")
        print(f"{'='*80}\n")
    
    print(f"\n{'='*70}")
    print(f"æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"ç»“æœæ±‡æ€»ï¼š")
    print(f" - æ—¥å¿—æ–‡ä»¶ï¼š{os.path.join(CONFIG['output_dir'], 'logs')}")
    print(f" - æœ€ä½³æ¨¡å‹ï¼š{os.path.join(CONFIG['output_dir'], 'best_models')}")
    print(f"{'='*70}")

if __name__ == "__main__":
    main()