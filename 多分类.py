import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import pandas as pd
import os
from PIL import Image
import numpy as np
from sklearn.metrics import classification_report
import warnings
warnings.filterwarnings('ignore')
import logging
from datetime import datetime

# ======================== 1. æ ¸å¿ƒé…ç½®ï¼ˆæ”¯æŒå¤šæ¨¡å‹+å¤šåˆ†ç±»ï¼‰ ========================
CONFIG = {
    # è·¯å¾„é…ç½®ï¼ˆä¿æŒä½ çš„åŸå§‹è·¯å¾„ï¼‰
    'train_csv': 'E:\\eyes__disease\\dataset\\Training_Set\\RFMiD_Training_Labels.csv',
    'train_img': 'E:\\eyes__disease\\dataset\\Training_Set\\Training',
    'val_csv': 'E:\\eyes__disease\\dataset\\Evaluation_Set\\RFMiD_Validation_Labels.csv',
    'val_img': 'E:\\eyes__disease\\dataset\\Evaluation_Set\\Validation',
    'test_csv': 'E:\\eyes__disease\\dataset\\Test_Set\\RFMiD_Testing_Labels.csv',
    'test_img': 'E:\\eyes__disease\\dataset\\Test_Set\\Test',
    
    # å¤šåˆ†ç±»ç›¸å…³é…ç½®
    'all_class_names': [],  # CSVç¬¬ä¸‰åˆ—å¼€å§‹çš„æ‰€æœ‰åˆ—ï¼ˆå«æ— æ ·æœ¬ç±»åˆ«ï¼‰
    'actual_class_names': [],  # å®é™…æœ‰æ ·æœ¬çš„ç±»åˆ«ï¼ˆå«Normalï¼‰
    'img_ext': '.png',          
    'img_size': (224, 224),    # InceptionV3å»ºè®®æ”¹ä¸º(299,299)ï¼Œå…¶ä»–æ¨¡å‹å…¼å®¹224
    'batch_size': 16,          # å¤šæ¨¡å‹é€‚é…ï¼ˆVGG16/InceptionV3å»ºè®®8ï¼›æ˜¾å­˜â‰¥12Gç”¨16ï¼‰
    'num_workers': 12,          
    'epochs': 50,              
    'lr': 1e-3,                # InceptionV3/SE-ResNet50å»ºè®®5e-5
    'weight_decay': 1e-5,       
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'output_dir': 'multiclass_model_comparison',  # å¤šæ¨¡å‹ç»“æœä¿å­˜æ ¹ç›®å½•
    'models_to_train': [        # å¯é€‰æ‹©è®­ç»ƒçš„å¤šåˆ†ç±»æ¨¡å‹ï¼ˆå¯å¢åˆ ï¼‰
        #'lenet5',
        'alexnet',
        #'vgg16',
        #'inception_v3',
        #'resnet50',
        #'densenet121',
        #'se_resnet50'
    ]
}

# ======================== 2. æ—¥å¿—é…ç½®ï¼ˆæ¯ä¸ªæ¨¡å‹ç‹¬ç«‹æ—¥å¿—ï¼Œä¾¿äºå¯¹æ¯”ï¼‰ ========================
def setup_logger(model_name):
    os.makedirs(CONFIG['output_dir'], exist_ok=True)
    log_dir = os.path.join(CONFIG['output_dir'], 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(log_dir, f'{model_name}_multiclass_train_{timestamp}.log')
    
    logger = logging.getLogger(model_name)
    logger.setLevel(logging.INFO)
    logger.propagate = False  # é¿å…é‡å¤è¾“å‡º
    
    # æ–‡ä»¶+æ§åˆ¶å°åŒè¾“å‡º
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    ))
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    return logger, log_file

# ======================== 3. ä»CSVç¬¬ä¸‰åˆ—æå–æ‰€æœ‰ç±»åˆ«ï¼ˆä¿ç•™ä½ çš„åŸé€»è¾‘ï¼‰ ========================
def extract_all_classes_from_col3(csv_path, logger):
    df = pd.read_csv(csv_path)
    disease_cols = df.columns[2:].tolist()  # ç¬¬ä¸‰åˆ—å¼€å§‹çš„æ‰€æœ‰ç–¾ç—…åˆ—
    CONFIG['all_class_names'] = ['Normal'] + disease_cols
    logger.info(f"âœ… æå–æ‰€æœ‰ç±»åˆ«ï¼ˆå«æ— æ ·æœ¬ï¼‰ï¼šå…±{len(CONFIG['all_class_names'])}ç±»")
    logger.info(f"ç±»åˆ«åˆ—è¡¨ï¼š{CONFIG['all_class_names']}")
    return df

# ======================== 4. è‡ªå®šä¹‰å¤šåˆ†ç±»æ•°æ®é›†ï¼ˆä¿ç•™ä½ çš„åŸé€»è¾‘ï¼‰ ========================
class RFMiDMulticlassDataset(Dataset):
    def __init__(self, csv_path, image_dir, transform=None):
        self.csv_df = pd.read_csv(csv_path)
        self.image_dir = image_dir
        self.transform = transform
        self.all_class_names = CONFIG['all_class_names']
        self.disease_cols = self.all_class_names[1:]  # æ’é™¤Normalçš„ç–¾ç—…åˆ—
        
        # å¤šåˆ†ç±»æ ‡ç­¾ç”Ÿæˆé€»è¾‘ï¼ˆå®Œå…¨ä¿ç•™ä½ çš„ä¿®å¤åé€»è¾‘ï¼‰
        self.img_ids = self.csv_df['ID'].values
        self.labels = []
        for _, row in self.csv_df.iterrows():
            if row['Disease_Risk'] == 0:
                self.labels.append(0)  # Normalç±»ï¼ˆæ ‡ç­¾0ï¼‰
            else:
                disease_label = -1
                for cls_idx, cls_name in enumerate(self.disease_cols, 1):
                    if row[cls_name] == 1:
                        disease_label = cls_idx
                        break
                # æ— åŒ¹é…ç–¾ç—…åˆ—æ—¶æ ‡è®°ä¸ºæœ€åä¸€ç±»
                self.labels.append(disease_label if disease_label != -1 else len(self.all_class_names)-1)
        
        # å›¾ç‰‡è·¯å¾„æ˜ å°„
        self.img_path_dict = {
            int(os.path.splitext(f)[0]): os.path.join(image_dir, f)
            for f in os.listdir(image_dir) if f.endswith(CONFIG['img_ext'])
        }

    def __len__(self):
        return len(self.csv_df)

    def __getitem__(self, idx):
        img_id = self.img_ids[idx]
        img_path = self.img_path_dict[img_id]
        
        # å¤„ç†æŸåå›¾ç‰‡
        try:
            image = Image.open(img_path).convert('RGB')
        except:
            image = Image.fromarray(np.random.randint(0, 255, size=CONFIG['img_size'] + (3,), dtype=np.uint8))
        
        if self.transform:
            image = self.transform(image)
        
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        return image, label

# ======================== 5. æ•°æ®é¢„å¤„ç†/å¢å¼ºï¼ˆä¿ç•™ä½ çš„åŸé€»è¾‘ï¼‰ ========================
def get_multiclass_transforms(train=True):
    transform_list = [
        transforms.Resize(CONFIG['img_size']),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ]
    
    if train:
        augmentations = [
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=20),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
        ]
        np.random.shuffle(augmentations)
        selected_augs = augmentations[:np.random.randint(2, 4)]
        transform_list = selected_augs + transform_list
    
    return transforms.Compose(transform_list)

# ======================== 6. åˆ›å»ºDataLoaderï¼ˆä¿ç•™ä½ çš„åŸé€»è¾‘+æ—¥å¿—è¾“å‡ºï¼‰ ========================
def create_multiclass_dataloaders(logger):
    train_transform = get_multiclass_transforms(train=True)
    val_test_transform = get_multiclass_transforms(train=False)
    
    # æ•°æ®é›†å®ä¾‹
    train_dataset = RFMiDMulticlassDataset(
        csv_path=CONFIG['train_csv'],
        image_dir=CONFIG['train_img'],
        transform=train_transform
    )
    val_dataset = RFMiDMulticlassDataset(
        csv_path=CONFIG['val_csv'],
        image_dir=CONFIG['val_img'],
        transform=val_test_transform
    )
    test_dataset = RFMiDMulticlassDataset(
        csv_path=CONFIG['test_csv'],
        image_dir=CONFIG['test_img'],
        transform=val_test_transform
    )
    
    # è·å–å®é™…æœ‰æ ·æœ¬çš„ç±»åˆ«ï¼ˆä¿ç•™ä½ çš„é€»è¾‘ï¼‰
    all_actual_labels = list(set(
        train_dataset.labels + val_dataset.labels + test_dataset.labels
    ))
    all_actual_labels.sort()
    CONFIG['actual_class_names'] = [CONFIG['all_class_names'][label] for label in all_actual_labels]
    
    logger.info(f"\nâœ… å®é™…æœ‰æ ·æœ¬çš„ç±»åˆ«ï¼šå…±{len(CONFIG['actual_class_names'])}ç±»")
    logger.info(f"å®é™…ç±»åˆ«åˆ—è¡¨ï¼š{CONFIG['actual_class_names']}")
    
    # DataLoader
    train_loader = DataLoader(
        train_dataset, batch_size=CONFIG['batch_size'],
        shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset, batch_size=CONFIG['batch_size']*2,
        shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset, batch_size=CONFIG['batch_size']*2,
        shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True
    )
    
    logger.info(f"\nâœ… DataLoaderä¿¡æ¯ï¼š")
    logger.info(f" - è®­ç»ƒé›†ï¼š{len(train_dataset)}æ ·æœ¬ | {len(train_loader)}æ‰¹æ¬¡")
    logger.info(f" - éªŒè¯é›†ï¼š{len(val_dataset)}æ ·æœ¬ | {len(val_loader)}æ‰¹æ¬¡")
    logger.info(f" - æµ‹è¯•é›†ï¼š{len(test_dataset)}æ ·æœ¬ | {len(test_loader)}æ‰¹æ¬¡")
    
    return train_loader, val_loader, test_loader

# ======================== 7. å¤šåˆ†ç±»æ¨¡å‹åˆ›å»ºï¼ˆæ ¸å¿ƒæ–°å¢ï¼š7ä¸ªæ¨¡å‹é€‚é…å¤šåˆ†ç±»ï¼‰ ========================
## è¾…åŠ©æ¨¡å—ï¼šSEæ¨¡å—ï¼ˆç”¨äºSE-Net 2017ï¼‰
class SEBlock(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)

def create_multiclass_model(model_name, num_classes, logger):
    """åˆ›å»ºå¤šåˆ†ç±»æ¨¡å‹ï¼ˆè¾“å‡ºç»´åº¦=æ‰€æœ‰ç±»åˆ«æ•°ï¼‰"""
    logger.info(f"ğŸ”§ åˆå§‹åŒ–å¤šåˆ†ç±»æ¨¡å‹ï¼š{model_name}ï¼ˆè¾“å‡ºç±»åˆ«æ•°ï¼š{num_classes}ï¼‰")
    
    if model_name == 'lenet5':
        # CNN 1998ï¼ˆLeNet-5ï¼‰ï¼šå¤šåˆ†ç±»é€‚é…
        class LeNet5(nn.Module):
            def __init__(self, num_classes):
                super(LeNet5, self).__init__()
                self.features = nn.Sequential(
                    nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=2),
                    nn.Tanh(),
                    nn.AvgPool2d(kernel_size=2, stride=2),
                    nn.Conv2d(6, 16, kernel_size=5, stride=1),
                    nn.Tanh(),
                    nn.AvgPool2d(kernel_size=2, stride=2),
                    nn.Conv2d(16, 120, kernel_size=5, stride=1),
                    nn.Tanh()
                )
                # é€‚é…224Ã—224è¾“å…¥ï¼ˆ2æ¬¡æ± åŒ–åï¼š224â†’112â†’56ï¼‰
                self.classifier = nn.Sequential(
                    nn.Linear(120 * 56 * 56, 84),
                    nn.Tanh(),
                    nn.Linear(84, num_classes)  # å¤šåˆ†ç±»è¾“å‡º
                )
            
            def forward(self, x):
                x = self.features(x)
                x = x.view(x.size(0), -1)
                x = self.classifier(x)
                return x
        
        model = LeNet5(num_classes=num_classes)
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šLeNet-5ï¼ˆ1998 CNNï¼‰| æ— é¢„è®­ç»ƒ | å·ç§¯Ã—3+æ± åŒ–Ã—2")
    
    elif model_name == 'alexnet':
        # AlexNet 2012ï¼šå¤šåˆ†ç±»é€‚é…
        model = models.alexnet(pretrained=True)
        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)  # 1000ç±»â†’å¤šåˆ†ç±»
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šAlexNetï¼ˆ2012ï¼‰| é¢„è®­ç»ƒæƒé‡ | ReLU+Dropout")
    
    elif model_name == 'vgg16':
        # VGGNet 2014ï¼šå¤šåˆ†ç±»é€‚é…
        model = models.vgg16(pretrained=True)
        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, num_classes)  # 1000ç±»â†’å¤šåˆ†ç±»
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šVGG16ï¼ˆ2014ï¼‰| é¢„è®­ç»ƒæƒé‡ | 3Ã—3å°å·ç§¯å †å ")
    
    elif model_name == 'inception_v3':
        # GoogLeNet/Inception 2014ï¼šå¤šåˆ†ç±»é€‚é…
        model = models.inception_v3(pretrained=True, aux_logits=False, transform_input=True)
        model.fc = nn.Linear(model.fc.in_features, num_classes)  # 1000ç±»â†’å¤šåˆ†ç±»
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šInceptionV3ï¼ˆ2014ï¼‰| é¢„è®­ç»ƒæƒé‡ | å¤šå°ºåº¦ç‰¹å¾èåˆ")
        logger.warning("âš ï¸  å»ºè®®ï¼šInceptionV3æœ€ä½³è¾“å…¥å°ºå¯¸299Ã—299ï¼Œå¯ä¿®æ”¹CONFIG['img_size']æå‡æ€§èƒ½")
    
    elif model_name == 'resnet50':
        # ResNet 2015ï¼šå¤šåˆ†ç±»é€‚é…
        model = models.resnet50(pretrained=True)
        num_ftrs = model.fc.in_features
        model.fc = nn.Linear(num_ftrs, num_classes)  # 1000ç±»â†’å¤šåˆ†ç±»
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šResNet50ï¼ˆ2015ï¼‰| é¢„è®­ç»ƒæƒé‡ | æ®‹å·®è¿æ¥")
    
    elif model_name == 'densenet121':
        # DenseNet 2017ï¼šå¤šåˆ†ç±»é€‚é…
        model = models.densenet121(pretrained=True)
        num_ftrs = model.classifier.in_features
        model.classifier = nn.Linear(num_ftrs, num_classes)  # 1000ç±»â†’å¤šåˆ†ç±»
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šDenseNet121ï¼ˆ2017ï¼‰| é¢„è®­ç»ƒæƒé‡ | å¯†é›†è¿æ¥+ç‰¹å¾å¤ç”¨")
    
    elif model_name == 'se_resnet50':
        # SE-Net 2017ï¼šå¤šåˆ†ç±»é€‚é…
        class SEBottleneck(nn.Module):
            expansion = 4
            def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):
                super(SEBottleneck, self).__init__()
                self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
                self.bn1 = nn.BatchNorm2d(planes)
                self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)
                self.bn2 = nn.BatchNorm2d(planes)
                self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)
                self.bn3 = nn.BatchNorm2d(planes * self.expansion)
                self.se = SEBlock(planes * self.expansion, reduction)
                self.relu = nn.ReLU(inplace=True)
                self.downsample = downsample
                self.stride = stride
            
            def forward(self, x):
                residual = x
                out = self.conv1(x)
                out = self.bn1(out)
                out = self.relu(out)
                out = self.conv2(out)
                out = self.bn2(out)
                out = self.relu(out)
                out = self.conv3(out)
                out = self.bn3(out)
                out = self.se(out)
                if self.downsample is not None:
                    residual = self.downsample(x)
                out += residual
                out = self.relu(out)
                return out
        
        # æ„å»ºSE-ResNet50ï¼ˆå¤šåˆ†ç±»è¾“å‡ºï¼‰
        from torchvision.models.resnet import ResNet
        model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)
        # åŠ è½½ResNet50é¢„è®­ç»ƒæƒé‡
        resnet50_pretrained = models.resnet50(pretrained=True)
        pretrained_state = resnet50_pretrained.state_dict()
        model_state = model.state_dict()
        pretrained_state = {k: v for k, v in pretrained_state.items() if k in model_state and 'se.' not in k}
        model_state.update(pretrained_state)
        model.load_state_dict(model_state)
        logger.info("âœ… æ¨¡å‹ç»†èŠ‚ï¼šSE-ResNet50ï¼ˆ2017ï¼‰| é¢„è®­ç»ƒæƒé‡ | é€šé“æ³¨æ„åŠ›+æ®‹å·®è¿æ¥")
    
    else:
        raise ValueError(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹ï¼š{model_name}ï¼ˆè¯·ä»CONFIG['models_to_train']ä¸­é€‰æ‹©ï¼‰")
    
    return model.to(CONFIG['device'])

# ======================== 8. å¤šåˆ†ç±»è¯„ä¼°å‡½æ•°ï¼ˆä¿ç•™ä½ çš„åŸé€»è¾‘+æ—¥å¿—è¾“å‡ºï¼‰ ========================
def evaluate_multiclass(model, loader, split_name, logger):
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(CONFIG['device']), labels.to(CONFIG['device'])
            outputs = model(images)
            preds = torch.argmax(outputs, dim=1)
            
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(labels.cpu().numpy())
    
    # è¿‡æ»¤æ— æ ·æœ¬ç±»åˆ«
    actual_labels = list(set(all_targets + all_preds))
    actual_labels.sort()
    actual_class_names = [CONFIG['all_class_names'][label] for label in actual_labels]
    
    # è¾“å‡ºåˆ†ç±»æŠ¥å‘Šï¼ˆæ—¥å¿—+æ§åˆ¶å°ï¼‰
    logger.info(f"\n{split_name} åˆ†ç±»æŠ¥å‘Šï¼ˆä»…æ˜¾ç¤ºæœ‰æ ·æœ¬çš„ç±»åˆ«ï¼‰ï¼š")
    report = classification_report(
        all_targets, all_preds,
        labels=actual_labels,
        target_names=actual_class_names,
        digits=2,
        zero_division=0
    )
    logger.info(report)
    print(report)  # æ§åˆ¶å°åŒæ­¥è¾“å‡º
    
    # è¿”å›æŠ¥å‘Šå­—å…¸ï¼ˆç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰
    return classification_report(
        all_targets, all_preds,
        labels=actual_labels,
        target_names=actual_class_names,
        output_dict=True,
        zero_division=0
    )

# ======================== 9. è®­ç»ƒå‡½æ•°ï¼ˆä¿ç•™ä½ çš„åŸé€»è¾‘+æ—¥å¿—è¾“å‡ºï¼‰ ========================
def train_one_epoch(model, loader, criterion, optimizer, epoch, logger):
    model.train()
    total_loss = 0.0
    
    for batch_idx, (images, labels) in enumerate(loader):
        images, labels = images.to(CONFIG['device']), labels.to(CONFIG['device'])
        
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * images.size(0)
        
        # æ¯10ä¸ªbatchæ‰“å°è¿›åº¦ï¼ˆæ—¥å¿—+æ§åˆ¶å°ï¼‰
        if (batch_idx + 1) % 10 == 0:
            log_msg = f"Epoch [{epoch+1}/{CONFIG['epochs']}] | Batch [{batch_idx+1}/{len(loader)}] | Loss: {loss.item():.4f}"
            logger.info(log_msg)
            print(log_msg)
    
    avg_loss = total_loss / len(loader.dataset)
    log_msg = f"Epoch [{epoch+1}] è®­ç»ƒæŸå¤±ï¼š{avg_loss:.4f}"
    logger.info(log_msg)
    print(log_msg)
    return avg_loss

# ======================== 10. å•ä¸ªæ¨¡å‹è®­ç»ƒæµç¨‹ï¼ˆé›†æˆæ—¥å¿—+æ¨¡å‹ä¿å­˜ï¼‰ ========================
def train_single_multiclass_model(model_name):
    # 1. åˆå§‹åŒ–æ—¥å¿—
    logger, log_file = setup_logger(model_name)
    logger.info(f"{'='*60}")
    logger.info(f"å¼€å§‹è®­ç»ƒå¤šåˆ†ç±»æ¨¡å‹ï¼š{model_name}")
    logger.info(f"è®­ç»ƒé…ç½®ï¼š{CONFIG}")
    logger.info(f"{'='*60}\n")
    
    try:
        # 2. æå–æ‰€æœ‰ç±»åˆ«
        extract_all_classes_from_col3(CONFIG['train_csv'], logger)
        
        # 3. åˆ›å»ºDataLoader
        train_loader, val_loader, test_loader = create_multiclass_dataloaders(logger)
        
        # 4. åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¤šåˆ†ç±»è¾“å‡ºç»´åº¦=æ‰€æœ‰ç±»åˆ«æ•°ï¼‰
        num_classes = len(CONFIG['all_class_names'])
        model = create_multiclass_model(model_name, num_classes, logger)
        
        # 5. åˆå§‹åŒ–æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨
        criterion = nn.CrossEntropyLoss()  # å¤šåˆ†ç±»æ ‡å‡†æŸå¤±
        optimizer = optim.AdamW(
            model.parameters(),
            lr=CONFIG['lr'],
            weight_decay=CONFIG['weight_decay']
        )
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)
        
        # 6. è®­ç»ƒè®°å½•ï¼ˆä¿å­˜æœ€ä½³æ¨¡å‹ï¼‰
        best_val_macro_f1 = 0.0
        model_save_dir = os.path.join(CONFIG['output_dir'], 'best_models')
        os.makedirs(model_save_dir, exist_ok=True)
        best_model_path = os.path.join(model_save_dir, f'{model_name}_multiclass_best.pth')
        
        # 7. è®­ç»ƒå¾ªç¯
        for epoch in range(CONFIG['epochs']):
            logger.info(f"\n{'='*40} Epoch {epoch+1}/{CONFIG['epochs']} {'='*40}")
            print(f"\n{'='*40} Epoch {epoch+1}/{CONFIG['epochs']} {'='*40}")
            
            # è®­ç»ƒ
            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, epoch, logger)
            
            # éªŒè¯
            val_report = evaluate_multiclass(model, val_loader, "Validation", logger)
            
            # åŸºäºå®å¹³å‡F1ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆé¿å…ç±»åˆ«ä¸å¹³è¡¡å½±å“ï¼‰
            val_macro_f1 = val_report['macro avg']['f1-score']
            if val_macro_f1 > best_val_macro_f1:
                best_val_macro_f1 = val_macro_f1
                torch.save({
                    'model_name': model_name,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_macro_f1': best_val_macro_f1,
                    'all_class_names': CONFIG['all_class_names'],
                    'actual_class_names': CONFIG['actual_class_names'],
                    'config': CONFIG
                }, best_model_path)
                logger.info(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯å®å¹³å‡F1: {best_val_macro_f1:.4f}ï¼‰åˆ°ï¼š{best_model_path}")
                print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯å®å¹³å‡F1: {best_val_macro_f1:.4f}ï¼‰")
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(train_loss)
        
        # 8. æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ï¼ˆåŠ è½½æœ€ä½³æ¨¡å‹ï¼‰
        logger.info(f"\n{'='*60}")
        logger.info(f"æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ï¼ˆåŠ è½½æœ€ä½³æ¨¡å‹ï¼‰")
        logger.info(f"{'='*60}")
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°ï¼ˆåŠ è½½æœ€ä½³æ¨¡å‹ï¼‰")
        print(f"{'='*60}")
        
        checkpoint = torch.load(best_model_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        evaluate_multiclass(model, test_loader, "Test", logger)
        
        # 9. è®­ç»ƒæ€»ç»“
        logger.info(f"\n{'='*60}")
        logger.info(f"{model_name} è®­ç»ƒæ€»ç»“ï¼š")
        logger.info(f" - æœ€ä½³éªŒè¯å®å¹³å‡F1ï¼š{best_val_macro_f1:.4f}")
        logger.info(f" - æœ€ä½³æ¨¡å‹è·¯å¾„ï¼š{best_model_path}")
        logger.info(f" - æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼š{log_file}")
        logger.info(f"{'='*60}")
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™ï¼š{str(e)}", exc_info=True)
        raise

# ======================== 11. å¤šæ¨¡å‹æ‰¹é‡è®­ç»ƒï¼ˆä¸»æµç¨‹ï¼‰ ========================
def main():
    print(f"{'='*70}")
    print(f"RFMiDå¤šåˆ†ç±»å¤šæ¨¡å‹æ¯”å¯¹è®­ç»ƒ")
    print(f"è®­ç»ƒæ¨¡å‹åˆ—è¡¨ï¼š{CONFIG['models_to_train']}")
    print(f"è®¾å¤‡ï¼š{CONFIG['device']} | è¾“å‡ºç›®å½•ï¼š{CONFIG['output_dir']}")
    print(f"{'='*70}\n")
    
    # å¾ªç¯è®­ç»ƒæ¯ä¸ªæ¨¡å‹
    for model_name in CONFIG['models_to_train']:
        print(f"\n{'='*80}")
        print(f"æ­£åœ¨è®­ç»ƒå¤šåˆ†ç±»æ¨¡å‹ï¼š{model_name}")
        print(f"{'='*80}")
        
        train_single_multiclass_model(model_name)
        
        print(f"\nâœ… {model_name} è®­ç»ƒå®Œæˆï¼ç»“æœä¿å­˜åˆ°ï¼š{CONFIG['output_dir']}")
        print(f"{'='*80}\n")
    
    print(f"\n{'='*70}")
    print(f"æ‰€æœ‰å¤šåˆ†ç±»æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"ç»“æœæ±‡æ€»ï¼š")
    print(f" - æ—¥å¿—æ–‡ä»¶ï¼š{os.path.join(CONFIG['output_dir'], 'logs')}")
    print(f" - æœ€ä½³æ¨¡å‹ï¼š{os.path.join(CONFIG['output_dir'], 'best_models')}")
    print(f"{'='*70}")

if __name__ == "__main__":
    main()